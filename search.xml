<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>论文速读-2023-5-12-语音识别</title>
      <link href="/blog/3163560131.html"/>
      <url>/blog/3163560131.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 语音识别 </category>
          
          <category> 论文速读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文速读 </tag>
            
            <tag> 语音识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>中文拼写检查任务方法初步简单总结</title>
      <link href="/blog/498304885.html"/>
      <url>/blog/498304885.html</url>
      
        <content type="html"><![CDATA[<h1 id="中文拼写检查任务方法初步简单总结-V1"><a href="#中文拼写检查任务方法初步简单总结-V1" class="headerlink" title="中文拼写检查任务方法初步简单总结 (V1)"></a>中文拼写检查任务方法初步简单总结 (V1)</h1><h2 id="一、任务简述"><a href="#一、任务简述" class="headerlink" title="一、任务简述"></a>一、任务简述</h2><p>中文拼写检查主要关注与对字或词级别的错误的纠正（并不涉及语法上的错误），任务的初步目标是寻找到出现错误的位置，然后是对错误进行纠正。</p><p>同时，为了简化问题，很多测评往往只关注如下情况的任务</p><ul><li>纠错目标多为音近字（多用于ASR结果的纠正）或者形近字（多用于OCR结果的纠正），不太考虑偶然情况的错误（但也会存在）</li><li>纠错前后语句长度不会发生变化（即不会发生增删）</li></ul><p>举例：</p><!-- ![img](https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230406214837237-972227706.png) --><div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230406214837237-972227706.png" alt="无法显示图片时显示的文字" style="zoom:0.9">    <br><!--换行-->    表一 拼写检查任务示例<!--标题-->    </center></div><br><p>但是，如果考虑真实数据集，文本纠错任务可能会遇到以下困难：</p><ul><li>一句文本中可能出现多出错误（此时会出现使用错误的上下文应用在语言模型中就行纠错的情况，增大纠错的难度）</li><li>常识性错误或命名实体性质的错误（比如“我打算暑假去金字塔旅游”—&gt;“我打算暑假去金字塔旅游”）或者如表一中第四条出现的情况，简单说就是正确的词用在了错误的地方。</li><li>训练集很难包含所有的可能错误情况，而让模型去纠正没有见过的错误是很困难的</li></ul><h2 id="二、数据准备工作"><a href="#二、数据准备工作" class="headerlink" title="二、数据准备工作"></a>二、数据准备工作</h2><h3 id="1-目前的开源数据集"><a href="#1-目前的开源数据集" class="headerlink" title="1.目前的开源数据集"></a>1.目前的开源数据集</h3><p><em><strong>(1) SIGHAN 2013-2015数据集</strong></em></p><p>该数据集的预料来源为中国台湾的留学生&#x2F;中小学生作文，修正语法、语义错误后得到源端文本，再经由人工 对错别字位置、纠正结果进行标注后获得目标端文本。初始数据集为繁体字，可以使用OpenCC等工具转换成简体字数据集（但转换过程中可能会较多的噪声）</p><p>同时，该数据集同时开源了一份混淆集，给出了汉字的易混淆字。</p><p><em><strong>(2) Wang 等人生成的一份伪数据</strong></em></p><p>该数据集是由人民日报和开源的中文演讲数据集加入错误后生成的伪数据集。</p><p><em><strong>(3) 测评开源数据集</strong></em></p><ul><li><p>本次测评开放的训练数据集（目前还未放出）</p><p>  <a href="https://github.com/Arvid-pku/NLPCC2023_Shared_Task8">https://github.com/Arvid-pku/NLPCC2023_Shared_Task8</a></p><p>  已开放，多为新闻文本</p></li><li><p>CCL-2022测评任务给出的数据集YACLC-CSC（数据量很小）<br><br><br></p></li></ul><p>数据集整合下载：<br>    <a href="https://blog.csdn.net/zhaohongfei_358/article/details/127093838">https://blog.csdn.net/zhaohongfei_358/article/details/127093838</a></p><p>提供了简体字版的SIGHAN数据集和Wang开源的伪数据集</p><h3 id="2-进行数据增强工作"><a href="#2-进行数据增强工作" class="headerlink" title="2.进行数据增强工作"></a>2.进行数据增强工作</h3><p>（1）考虑字音相似生成数据</p><p>根据文字的拼音，根据声母、韵母的相似度分别生成同声字符集、同韵字符集。再由得到的字符集，随机替换正确文本语录，得到训练用的语料库。</p><p>（2）按一定规则根据混淆集随机替换正确语料（或者已有CSC任务语料）得到扩充数据集。（CCL-2022 rank1所使用方法）</p><p>具体规则如下：</p><ol start="0"><li>可使用维基百科数据集、微信语料库、新闻语料库等作为原始未标注数据集</li><li>使用分词算法将原句子进行分词处理。</li><li>使用序列标注模型对所有词语进行属性标注、</li><li>对被序列标注模型所标注出的人名，地名类词语(如坸坸酒店，坸坸公司)进行不设错处理， 即不会被替换为错字。同样被过滤的还有非中文词和停用词。</li><li>随机按比例抽取字词进行改动。如该词在混淆集中，15%不改动，15%概率随机改动，70%概率在混淆集中随机抽取改动。如该词不在混淆集中则不改动。</li></ol><h3 id="3-辅助数据集"><a href="#3-辅助数据集" class="headerlink" title="3.辅助数据集"></a>3.辅助数据集</h3><p>（1）谐音字混淆集</p><p><a href="https://github.com/LiangsLi/ChineseHomophones">https://github.com/LiangsLi/ChineseHomophones</a></p><p>（2）形近字混淆集</p><p><a href="https://github.com/wdimmy/Automatic-Corpus-Generation/blob/master/corpus/confusion.txt">https://github.com/wdimmy/Automatic-Corpus-Generation/blob/master/corpus/confusion.txt</a></p><p>（3）字音字形混淆数据集</p><p><a href="https://raw.githubusercontent.com/ACL2020SpellGCN/SpellGCN/master/data/gcn_graph.ty_xj/spellGraphs.txt">https://raw.githubusercontent.com/ACL2020SpellGCN/SpellGCN/master/data/gcn_graph.ty_xj/spellGraphs.txt</a></p><p><a href="https://github.com/FDChongLi/TwoWaysToImproveCSC/tree/main/SoftmaskedBert/save">https://github.com/FDChongLi/TwoWaysToImproveCSC/tree/main/SoftmaskedBert/save</a></p><p>（4）字音特征</p><p><a href="https://github.com/whgaara/tensorflow-faspell/blob/master/data/char_meta.txt">https://github.com/whgaara/tensorflow-faspell/blob/master/data/char_meta.txt</a></p><p><a href="https://link.csdn.net/?target=https://unicode.org/Public/UNIDATA/Unihan.zip">https://link.csdn.net/?target=https%3A%2F%2Funicode.org%2FPublic%2FUNIDATA%2FUnihan.zip</a></p><p>（5）字形特征（拆字）</p><p><a href="https://raw.githubusercontent.com/cjkvi/cjkvi-ids/master/ids.txt">https://raw.githubusercontent.com/cjkvi/cjkvi-ids/master/ids.txt</a></p><p>注：github上还有其他的开源的混淆集等数据集，同时使用开源工具pypinyin可以得到字符的拼音序列。</p><h2 id="三、输入输出与模型"><a href="#三、输入输出与模型" class="headerlink" title="三、输入输出与模型"></a>三、输入输出与模型</h2><h3 id="1-输入特征"><a href="#1-输入特征" class="headerlink" title="1.输入特征"></a>1.输入特征</h3><p>对于输入，多使用三方面的特征</p><ul><li>字符序列</li><li>拼音特征</li><li>字形特征</li></ul><h4 id="1-字符特征的编码"><a href="#1-字符特征的编码" class="headerlink" title="(1) 字符特征的编码"></a>(1) 字符特征的编码</h4><p>将字符序列转化为特征序列后，与其他特征融合，作为输入（一般使用词向量，也有使用编码器，如bert，编码成特征序列）</p><h4 id="2-拼音特征的使用"><a href="#2-拼音特征的使用" class="headerlink" title="(2) 拼音特征的使用"></a>(2) 拼音特征的使用</h4><p>首先，要将拼音序列补齐或截断成相同长度的序列，然后再进行进一步处理</p><ul><li>使用卷积神经网络提取特征，方法如下图</li></ul><div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407085218865-1377330475.png" alt="无法显示图片时显示的文字" style="zoom:0.6">    <br><!--换行-->    <!--标题-->    </center></div><br>* 使用编码器编码特征<div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407100215784-1669350069.png" alt="无法显示图片时显示的文字" style="zoom:0.6">    <br><!--换行-->    <!--标题-->    </center></div><br>#### (3) 字形特征的使用<ul><li>使用卷积神经网络提取特征</li></ul><div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407100314024-266818179.png" alt="无法显示图片时显示的文字" style="zoom:0.6">    <br><!--换行-->    <!--标题-->    </center></div><br><ul><li><p>对拆字后对结果进行编码</p><p>  基于treeLSTM模型结构，用字的IDS拆分结构作为treeLSTM的输入，学习字向量表示</p></li></ul><div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407104723809-1109818543.png" alt="无法显示图片时显示的文字" style="zoom:0.5">    <br><!--换行-->    <!--标题-->    </center></div><br><ul><li><p>字形嵌入</p><p>  受Glyce的启发，有人使用包含汉字的位图信息的字体文件，例如使用仿宋、行楷和楷书3种中文字体，并将每种字体实例化成大小为24*24的图像，将三种字体的信息通过嵌入层转化成一维表示，再通过全连接层得到字形嵌入。</p></li></ul><h4 id="4-特征的融合"><a href="#4-特征的融合" class="headerlink" title="(4) 特征的融合"></a>(4) 特征的融合</h4><ul><li><p>SpellGCN 提出使用图神经网络对字音和字形结构关系进行学习，并且将这种字音字形的向量融入到字的embedding中，在纠错分类的时候，纠错更倾向于预测为混淆集里的字。</p><p><img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407105649139-1632220282.png" alt="img"></p></li><li><p>使用全连接层融合特征</p></li></ul><!-- ![img](https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407111355129-24589655.png) --><div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407111355129-24589655.png" alt="无法显示图片时显示的文字" style="zoom:0.4">    <br><!--换行-->        <!--标题-->    </center></div><br><ul><li><p>基于错误概率的特征融合</p><p>  其中pi是检错网络计算出的每个字的错误概率，ei为词嵌入特征，efi为语音和字形特征</p></li></ul><!-- ![img](https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407111753798-583497888.png) --><div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407111753798-583497888.png" alt="无法显示图片时显示的文字" style="zoom:0.5">    <br><!--换行-->        <!--标题-->    </center></div><br><h3 id="2-常用的模型"><a href="#2-常用的模型" class="headerlink" title="2.常用的模型"></a>2.常用的模型</h3><h4 id="1-编码器解码器结构"><a href="#1-编码器解码器结构" class="headerlink" title="(1) 编码器解码器结构"></a>(1) 编码器解码器结构</h4><div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407112447437-1400894155.png" alt="无法显示图片时显示的文字" style="zoom:0.3">    <br><!--换行-->        编码器解码器结构的文本纠错<!--标题-->    </center></div><br><p>如图所示，把文本拼写检查纠正任务当作文本翻译任务，编码器编码待纠错文本，解码器解码得到纠正后的文本。</p><h4 id="2-基于bert的结构"><a href="#2-基于bert的结构" class="headerlink" title="(2) 基于bert的结构"></a>(2) 基于bert的结构</h4><p>由bert生成候选集，再经过过滤网络从后续集中挑选合适的纠正结果（比如：选bert生成概率前k个概率中，包含在混淆集中且最大的概率，若前k个中不存在有在混淆集中的，则不进行纠错）</p><div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407113242756-1419458491.png" alt="无法显示图片时显示的文字" style="zoom:0.3">    <br><!--换行-->        基于bert+置信度相似度解码<!--标题-->    </center></div><br><h4 id="3-分为检错网络和纠错网络"><a href="#3-分为检错网络和纠错网络" class="headerlink" title="(3) 分为检错网络和纠错网络"></a>(3) 分为检错网络和纠错网络</h4><p>举例1:</p><!-- ![img](https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407113446060-1855604480.png) --><div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407113446060-1855604480.png" alt="无法显示图片时显示的文字" style="zoom:0.3">    <br><!--换行-->        检错网络<!--标题-->    </center></div><br><!-- ![img](https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407113606025-1802017176.png) --><div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407113606025-1802017176.png" alt="无法显示图片时显示的文字" style="zoom:0.4">    <br><!--换行-->        纠错网络<!--标题-->    </center></div><br><p>举例2:</p><div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407113750868-349242122.png" alt="无法显示图片时显示的文字" style="zoom:0.3">    <br><!--换行-->        Soft-Mask BERT （字节跳动)<!--标题-->    </center></div><br><p>在错误检测部分，通过BiGRU模型对每个输入字符进行错误检测，得到每个输入字符的错误概率值参与计算soft-masked embedding作为纠错部分的输入向量，一定程度减少了bert模型的过纠问题，提高纠错准确率。</p><h2 id="四、CCL-2022-前三方案"><a href="#四、CCL-2022-前三方案" class="headerlink" title="四、CCL-2022 前三方案"></a>四、CCL-2022 前三方案</h2><h3 id="1-rank1-方案"><a href="#1-rank1-方案" class="headerlink" title="1. rank1 方案"></a>1. rank1 方案</h3><p><img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407150433583-1944262366.png" alt="img"></p><center> 整体流程图 </center><br><div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407150934765-1994696061.png" alt="无法显示图片时显示的文字" style="zoom:0.3">    <br><!--换行-->        模型结构<!--标题-->    </center></div><br><p>主要使用了以下方法：</p><p>（1）结合拼音特征和字符序列特征</p><p>（2）基本使用base-bert模型</p><p>（3）为了解决一句话中存在多处错误的问题，采用多轮纠错的方法，每次只纠正一个错误。</p><p>（4）为了解决“用正确的字替换正确的字”的情况，通过计算修改前后的困惑度来减少误召回情况</p><p>（5）为了解决命名实体的纠正问题，训练了一个基于bert+crf的序列标准模型</p><p>（6）最后训练了一个严格的n-gram模型，来进行最后的纠正</p><h3 id="2-rank2-方案"><a href="#2-rank2-方案" class="headerlink" title="2. rank2 方案"></a>2. rank2 方案</h3><p>（1）该方案主要尝试了ReaLiSe (Xu et al., 2021)、CRASpell (Liu et al., 2022)、Macbert4csc (Cui et al., 2020)、SpellGCN (Cheng et al., 2020)、ReaLiSe算法等多个模型，并对结果进行了融合。</p><p>（2）融合方法：把各个模型的纠错结果加入候选集，通过计算每个候选词的得分，从而得到最优结果。通过调研，决定采用N-gram计算替换每个候选词后句子的困惑度得分来决定选择最终结果。分别训练了字级别的5-gram模型和词级别 的3-gram模型。在句子的困惑度的得分计算时，综合考虑字级别和词级别的得分，以此提高结 果的可信度。</p><p>（2）将传统CSC任务的Detection阶段单独拆出来做一个简单的判断句子中每个位置 对错的二分类任务，以此来提高模型的检错性能，尽可能的把所有的错误位置都检查出来。 在纠错阶段我们结合N-gram计算困惑度进行纠错，同时设置了一些过滤规则，这样引入了 一些额外知识保证纠错阶段的可控性，也使得纠错阶段的性能得以提高</p><h3 id="3-rank3-方案"><a href="#3-rank3-方案" class="headerlink" title="3. rank3 方案"></a>3. rank3 方案</h3><br><div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407154533986-527061261.png" alt="无法显示图片时显示的文字" style="zoom:0.4">    <br><!--换行-->        模型结构<!--标题-->    </center></div><br><p>(1) 在使用bert的同时，将预训练时mlm任务的全连接层参数也作为初始化参数</p><p>(2) 使用多个模型，多个模型的词表保持一致，将全连接层输 出的概率进行平均</p><p>(3) 设置规则，对非音近，形近的修改，更加严格。</p><h2 id="四、baseline"><a href="#四、baseline" class="headerlink" title="四、baseline"></a>四、baseline</h2><p>在ccl-track1任务中，提供了一份基于base-bert实现的baseline,该代码直接是使用了预训练的bert模型，将编码结果经过全连接层并softmax后的输出作为概率，进行输出。</p><p>项目结构：</p><div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202304/1203819-20230407154818074-679524798.png" alt="无法显示图片时显示的文字" style="zoom:0.6">    <br><!--换行-->        项目结构<!--标题-->    </center></div><br><p>其中,data_preprocess.py负责处理原始数据，train.py负责训练，decode.py负责预测。</p><p><em><strong>todo-list:</strong></em></p><p>目前nlpcc-task8已经开放了原始训练数据，但是原始数据无法直接用于该baseline。需要做一下两个工作：</p><ul><li>将训练数据拆解成src tgt两份文件，并标注id</li><li>生成一份文件，内容为纠正错误的位置已经纠正前后的内容</li></ul><h2 id="五、相关链接"><a href="#五、相关链接" class="headerlink" title="五、相关链接"></a>五、相关链接</h2><ol><li>nlpcc-task8 数据集<br><a href="https://github.com/Arvid-pku/NLPCC2023_Shared_Task8">https://github.com/Arvid-pku/NLPCC2023_Shared_Task8</a></li><li>文本纠错资源集<br><a href="https://github.com/destwang/CTCResources#chinese-spelling-check-csc">https://github.com/destwang/CTCResources#chinese-spelling-check-csc</a></li><li>CTC-2021比赛<br><a href="https://github.com/destwang/CTC2021">https://github.com/destwang/CTC2021</a></li><li>CCL-2022测评报告<br><a href="https://blcuicall.org/CCL2022-CLTC/report/">https://blcuicall.org/CCL2022-CLTC/report/</a></li><li>中文纠错（Chinese Spelling Correct） 最新技术方案总结<br><a href="https://blog.csdn.net/javastart/article/details/122511809">https://blog.csdn.net/javastart/article/details/122511809</a></li><li>基于BERT和多特征融合嵌入的中文拼写检查<br><a href="https://www.jsjkx.com/CN/10.11896/jsjkx.220100104">https://www.jsjkx.com/CN/10.11896/jsjkx.220100104</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>基于神经网络的语言模型（简单模型）</title>
      <link href="/blog/352722672.html"/>
      <url>/blog/352722672.html</url>
      
        <content type="html"><![CDATA[<h1 id="基于神经网络的语言模型（简单模型）"><a href="#基于神经网络的语言模型（简单模型）" class="headerlink" title="基于神经网络的语言模型（简单模型）"></a>基于神经网络的语言模型（简单模型）</h1><blockquote><p>本文主要为学习《自然语言处理：基于预训练模型的方法》第五章学习的总结，主要是使用全连接神经网络、循环神经网络来实现P(Xi|Xi-1Xi-2…Xi-m)</p></blockquote><h2 id="一、加载数据集"><a href="#一、加载数据集" class="headerlink" title="一、加载数据集"></a>一、加载数据集</h2><p>本次使用是NLTK包中提供的<code>reuters</code>语料库。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_reuters</span>():</span><br><span class="line">    <span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> reuters</span><br><span class="line">    text = reuters.sents()</span><br><span class="line">    <span class="comment">#print(text[1])</span></span><br><span class="line">    text = [[word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> sentence] <span class="keyword">for</span> sentence <span class="keyword">in</span> text]</span><br><span class="line">    vocab = Vocab.build(text,reserved_tokens=[PAD_TOKEN,BOS_TOKEN,EOS_TOKEN])</span><br><span class="line">    corpus = [vocab.covert_tokens_to_ids(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> text]</span><br><span class="line">    <span class="keyword">return</span> corpus,vocab</span><br></pre></td></tr></table></figure><p>加载文本数据集，构建语料库类和词表类，并返回。<br>其中<code>corpus</code>是一个由token的id组成的每一句话的列表，<code>vocab</code>为根据语料库所有出现过的词构建的词表，并为每个词（token）赋予一个独有索引，并支持查询token和index的互相转换。</p><h2 id="二、基于前馈神经网络的语言模型"><a href="#二、基于前馈神经网络的语言模型" class="headerlink" title="二、基于前馈神经网络的语言模型"></a>二、基于前馈神经网络的语言模型</h2><h3 id="1-构建数据集"><a href="#1-构建数据集" class="headerlink" title="1.构建数据集"></a>1.构建数据集</h3>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
          <category> 语言模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 语言模型 </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于神经网络的简单文本分类（情感分类）总结</title>
      <link href="/blog/340717617.html"/>
      <url>/blog/340717617.html</url>
      
        <content type="html"><![CDATA[<h1 id="基于神经网络的简单文本分类（情感分类）总结"><a href="#基于神经网络的简单文本分类（情感分类）总结" class="headerlink" title="基于神经网络的简单文本分类（情感分类）总结"></a>基于神经网络的简单文本分类（情感分类）总结</h1><blockquote><p>本文主要为学习《自然语言处理：基于预训练模型的方法》第四章学习的总结</p></blockquote><h2 id="一、任务描述"><a href="#一、任务描述" class="headerlink" title="一、任务描述"></a>一、任务描述</h2><p>主要使用NLTK提供的句子倾向性分析数据（sentence_polarity）作为数据集，尝试使用多层感知机模型、卷积神经网络、循环神经网络、Transformer等模型应用在该数据集上，查看模型的分类效果。</p><!-- ![数据集一条sample的样式](https://img2023.cnblogs.com/blog/1203819/202303/1203819-20230328221127468-1249202682.png) --><div><!--块级封装-->    <center><!--将图片和文字居中-->    <img src="https://img2023.cnblogs.com/blog/1203819/202303/1203819-20230328221127468-1249202682.png" alt="无法显示图片时显示的文字" style="zoom:0.9">    <br><!--换行-->    数据集中一条sample的样式<!--标题-->    </center></div><h2 id="二、加载数据以及前期准备工作"><a href="#二、加载数据以及前期准备工作" class="headerlink" title="二、加载数据以及前期准备工作"></a>二、加载数据以及前期准备工作</h2><h3 id="1-加载数据"><a href="#1-加载数据" class="headerlink" title="1.加载数据"></a>1.加载数据</h3><p>在NLTK中，提供了可以可以直接加载数据的API，直接按pos和neg两种标签加载数据，并划分前4000条为训练集，剩下的为测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_sentence_polarity</span>():</span><br><span class="line">    <span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> sentence_polarity</span><br><span class="line">    vocab = Vocab.build(sentence_polarity.sents())</span><br><span class="line">    <span class="built_in">print</span>(sentence_polarity.sents()[:<span class="number">1</span>])</span><br><span class="line">    train_data = [(vocab.covert_tokens_to_ids(sentence),<span class="number">0</span>) <span class="keyword">for</span> sentence</span><br><span class="line">                    <span class="keyword">in</span> sentence_polarity.sents(categories=<span class="string">&#x27;pos&#x27;</span>)[:<span class="number">4000</span>]] \</span><br><span class="line">                    + [(vocab.covert_tokens_to_ids(sentence),<span class="number">1</span>) <span class="keyword">for</span> sentence</span><br><span class="line">                    <span class="keyword">in</span> sentence_polarity.sents(categories=<span class="string">&#x27;neg&#x27;</span>)[:<span class="number">4000</span>]]</span><br><span class="line">    test_data = [(vocab.covert_tokens_to_ids(sentence),<span class="number">0</span>) <span class="keyword">for</span> sentence</span><br><span class="line">                    <span class="keyword">in</span> sentence_polarity.sents(categories=<span class="string">&#x27;pos&#x27;</span>)[<span class="number">4000</span>:]] \</span><br><span class="line">                    + [(vocab.covert_tokens_to_ids(sentence),<span class="number">1</span>) <span class="keyword">for</span> sentence</span><br><span class="line">                    <span class="keyword">in</span> sentence_polarity.sents(categories=<span class="string">&#x27;neg&#x27;</span>)[<span class="number">4000</span>:]]</span><br><span class="line">    <span class="comment">#print(len(train_data),len(test_data))</span></span><br><span class="line">    <span class="keyword">return</span> BowDataset(train_data),BowDataset(test_data),vocab </span><br></pre></td></tr></table></figure><h3 id="2-构建词表"><a href="#2-构建词表" class="headerlink" title="2.构建词表"></a>2.构建词表</h3><p>在自然语言处理中，我们可以把像词、字、字母这种处理单位统称为token，深度学习中训练过程多为数值计算，因此无法直接处理这些token。为此，对于每个token，我们需要为其赋予一个独有的索引值来代表这个token，这样才可以用于后续的学习训练。</p><p>为此，我们构建了<code>Vocab</code>词表类来为语料中所有出现过的token来赋予索引。<code>Vocab</code>类主要实现了以下几个功能：</p><ul><li>将一个token转化为对应的索引值</li><li>将一个token序列（即一句话）转化成索引值序列</li><li>通过索引值获得对应的token</li><li>读入语料库，为每个token赋予独有的索引值，同时为未登录词<code>&lt;unk&gt;</code>和填充词<code>&lt;pad&gt;</code>设置索引值。</li></ul><p>具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Vocab</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,tokens=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.idx_to_token = <span class="built_in">list</span>()</span><br><span class="line">        self.token_to_idx = <span class="built_in">dict</span>()</span><br><span class="line">        self.unk = <span class="string">&quot;&lt;unk&gt;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> tokens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> self.unk <span class="keyword">not</span> <span class="keyword">in</span> tokens:</span><br><span class="line">                tokens = tokens + self.unk</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">                self.idx_to_token.append(token)</span><br><span class="line">                self.token_to_idx[token] = <span class="built_in">len</span>(self.idx_to_token) - <span class="number">1</span></span><br><span class="line">        self.unk_idx = self.token_to_idx.get(self.unk)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">cls,text,min_freq=<span class="number">1</span>,reserved_tokens=<span class="literal">None</span></span>):</span><br><span class="line">        tokens_freq = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> text:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">                tokens_freq[word] += <span class="number">1</span></span><br><span class="line">        tokens = [<span class="string">&quot;&lt;unk&gt;&quot;</span>] + (reserved_tokens <span class="keyword">if</span> reserved_tokens <span class="keyword">else</span> [])</span><br><span class="line">        tokens += [token <span class="keyword">for</span> token,freq <span class="keyword">in</span> tokens_freq.items() </span><br><span class="line">                        <span class="keyword">if</span> freq&gt;=min_freq <span class="keyword">and</span> token != <span class="string">&quot;&lt;unk&gt;&quot;</span>]</span><br><span class="line">        <span class="keyword">return</span> cls(tokens)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,token</span>):</span><br><span class="line">        <span class="keyword">return</span> self.token_to_idx.get(token,self.unk_idx)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">covert_tokens_to_ids</span>(<span class="params">self,tokens</span>):</span><br><span class="line">        <span class="keyword">return</span> [self[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">covert_ids_to_tokens</span>(<span class="params">self,indices</span>):</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> indices]</span><br></pre></td></tr></table></figure><p>其中，<code>@classmethod</code>的作用是指定该方法为类方法，而不是实例方法。同时，被<code>@classmethod</code>传入的第一个参数可以是当前类。该修饰器的具体方法可以参考<br><a href="https://blog.csdn.net/leviopku/article/details/100745811">https://blog.csdn.net/leviopku/article/details/100745811</a></p><h3 id="3-构建数据集类"><a href="#3-构建数据集类" class="headerlink" title="3.构建数据集类"></a>3.构建数据集类</h3><h3 id="4-得到测试集和训练集"><a href="#4-得到测试集和训练集" class="headerlink" title="4.得到测试集和训练集"></a>4.得到测试集和训练集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_sentence_polarity</span>():</span><br><span class="line">    <span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> sentence_polarity</span><br><span class="line">    vocab = Vocab.build(sentence_polarity.sents())</span><br><span class="line">    <span class="built_in">print</span>(sentence_polarity.sents()[:<span class="number">1</span>])</span><br><span class="line">    train_data = [(vocab.covert_tokens_to_ids(sentence),<span class="number">0</span>) <span class="keyword">for</span> sentence</span><br><span class="line">                    <span class="keyword">in</span> sentence_polarity.sents(categories=<span class="string">&#x27;pos&#x27;</span>)[:<span class="number">4000</span>]] \</span><br><span class="line">                    + [(vocab.covert_tokens_to_ids(sentence),<span class="number">1</span>) <span class="keyword">for</span> sentence</span><br><span class="line">                    <span class="keyword">in</span> sentence_polarity.sents(categories=<span class="string">&#x27;neg&#x27;</span>)[:<span class="number">4000</span>]]</span><br><span class="line">    test_data = [(vocab.covert_tokens_to_ids(sentence),<span class="number">0</span>) <span class="keyword">for</span> sentence</span><br><span class="line">                    <span class="keyword">in</span> sentence_polarity.sents(categories=<span class="string">&#x27;pos&#x27;</span>)[<span class="number">4000</span>:]] \</span><br><span class="line">                    + [(vocab.covert_tokens_to_ids(sentence),<span class="number">1</span>) <span class="keyword">for</span> sentence</span><br><span class="line">                    <span class="keyword">in</span> sentence_polarity.sents(categories=<span class="string">&#x27;neg&#x27;</span>)[<span class="number">4000</span>:]]</span><br><span class="line">    <span class="comment">#print(len(train_data),len(test_data))</span></span><br><span class="line">    <span class="keyword">return</span> BowDataset(train_data),BowDataset(test_data),vocab </span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/blog/1243066710.html"/>
      <url>/blog/1243066710.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
